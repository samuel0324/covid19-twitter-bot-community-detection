{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp6ZRImV_Mm2"
   },
   "source": [
    "# Detecting Bots in Early COVID-19 Tweets Using KMeans Clustering\n",
    "\n",
    "# Data Collection\n",
    "\n",
    "### Samuel Park\n",
    "### October 25th, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU746c-IDBkR"
   },
   "source": [
    "*Note that this notebook was run on a local machine. \n",
    "\n",
    "*Next notebook will be run on Google Colab primarily.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "to1lLczcQLm3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "# from twarc import Twarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IImDaapsQLnR"
   },
   "source": [
    "\n",
    "\n",
    "## Coronavirus (covid19) Tweets - Early and Late April\n",
    "Dataset posted by [Shane Smith](https://www.kaggle.com/smid80/coronavirus-covid19-tweets-early-april) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee09UbJDCaHP"
   },
   "source": [
    "The datset contains COVID-19-related tweets between March 29th, 2020, and April 30th, 2020, when the pandemic was beginning in North America. The publisher had uploaded individual CSV file for each date between that timeframe. So, those CSV files have to be concantenated row-wise.\n",
    "\n",
    "When collecting COVID-19-related tweets, the publisher used these hashtags: \n",
    "- #coronavirus \n",
    "- #coronavirusoutbreak\n",
    "- #coronavirusPandemic, \n",
    "- #covid19\n",
    "- #covid_19\n",
    "- #epitwitter\n",
    "- #ihavecorona\n",
    "- #StayHomeStaySafe\n",
    "- #TestTraceIsolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tTlCoeL8QLnR",
    "outputId": "045c7638-875d-43f8-cc15-ed44e8c2dd04",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in 2020-03-29 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-03-29 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-03-30 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-03-30 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-03-31 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-03-31 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-01 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-01 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-02 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-02 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-03 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-03 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-04 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-04 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-05 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-05 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-06 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-06 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-07 Coronavirus Tweets.CSV\n",
      "appended dataframe of 2020-04-07 Coronavirus Tweets.CSV to df_list\n",
      "reading in 2020-04-08 Coronavirus Tweets.CSV\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d61cb47de74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# just to show progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'reading in {filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/shane_smith/{filename}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# df = pd.concat([df, df_temp], axis=0, ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'data/shane_smith'\n",
    "\n",
    "# create a list of all the CSV files from Shane Smith\n",
    "all_files = [file for file in listdir(path)]\n",
    "all_files = sorted(all_files) # sort by date\n",
    "\n",
    "# store \n",
    "df_list = []\n",
    "\n",
    "# tweets = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    # just to show progress\n",
    "    print(f'reading in {filename}')\n",
    "    df_temp = pd.read_csv(f'data/shane_smith/{filename}', index_col=None, header=0)\n",
    "    # df = pd.concat([df, df_temp], axis=0, ignore_index=True)\n",
    "    df_list.append(df_temp)\n",
    "    print(f'appended dataframe of {filename} to df_list')\n",
    "    \n",
    "tweets = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvFi-aCOQLnS",
    "outputId": "37280838-4119-44cc-a6ff-383936220661",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsHWvHVlQLnS"
   },
   "outputs": [],
   "source": [
    "# export the compiled df as csv file\n",
    "tweets.to_csv(path_or_buf='data/april_tweets_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbmTWCWWBskG"
   },
   "source": [
    "Preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDctzg3tQLnS",
    "outputId": "91756801-2f14-4915-c370-8af9d9484620",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the first 5 rows\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSskKMdGQLnS",
    "outputId": "84dcbd80-16b5-48ef-f998-20d0182ce77e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show data types of the columns\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec0Rc_sNQLnT"
   },
   "source": [
    "`created_at` column should be in datetime format. `account_lang` is in `float64` format, but intuitively it should be in `object` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HINenez2QLnT"
   },
   "source": [
    "Column descriptions:\n",
    "- status_id: unique id for each tweet\n",
    "- user_id: unique ide for a user\n",
    "- created_at: datetime for when the tweet was created\n",
    "- screen_name: user's Twitter handle\n",
    "- text: content of the tweet\n",
    "- source: link to the tweet\n",
    "- reply_to_status_id: tweet id to which this tweet is a reply\n",
    "- reply_to_user_id: user id to which this tweet is a reply\n",
    "- is_quote: whether or not this tweet is a quote\n",
    "- is_retweet: whether or not this tweet is a retweet\n",
    "- favourites_count: number of users who favorited this tweet\n",
    "- retweet_count: number of times this tweet was retweeted\n",
    "- country_code: two-letter code for countries\n",
    "- place_ful_name: full name of the place if geo-tagged (New York, NY)\n",
    "- place_type: urban or rural? Can be NaN if not geo-tagged\n",
    "- followers_count: number of followers this user has\n",
    "- friends_count: number of people this user follows\n",
    "- account_lang: the language setting of the user account\n",
    "- acount_created_at: the date when the account was created\n",
    "- verified: whether or not the user is verified (i.e., has blue check mark or not)\n",
    "- lang: the language of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIAUDg56QLnY"
   },
   "source": [
    "Let's get the unique language codes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8tmsAfZQLnY",
    "outputId": "abe74ab8-21a5-46c9-f1b7-a1efa6b13d02",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets['lang'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5V85XOFQLnZ"
   },
   "source": [
    "As shown, there are many languages in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhajzobMQLnZ"
   },
   "source": [
    "Let's only get the rows whose `lang` values are `en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adt7P4sSQLna",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select rows whose lang is en\n",
    "tweets_en = tweets.loc[tweets['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1pIpuzJQLna",
    "outputId": "cfcf83fc-0f90-4396-af6f-f98d5510a228",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_jRPXypQLnZ"
   },
   "source": [
    "It looks like there are 8,133,785 tweets whose language code is \"en\" (i.e., English). We want to keep only English tweets for this analysis. There are three reasons for this:\n",
    "- We are primarily concerned with any presence of bots that may have influenced English-speakers. \n",
    "- We may do natural language processing down the line. English language has lots of references to utilize.\n",
    "- The author of this notebook is fluent in a very limited number of languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqfX5drRAgar"
   },
   "source": [
    "Export the English-only tweets as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opdG5Ag0QLna"
   },
   "outputs": [],
   "source": [
    "tweets_en.to_csv('data/tweets_en_colab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OikL1jlpQLna"
   },
   "source": [
    "`tweets_en` which includes only English tweets contains 8,133,785 rows (tweets) and 21 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UoV8b7JAvrK"
   },
   "source": [
    "Data cleaning will be continued in the second notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "capstone_samuel_park_notebook1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
